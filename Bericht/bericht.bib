@mastersthesis{woudenberg_chatbot_2014,
  title      = {A {Chatbot} {Dialogue} {Manager}-{Chatbots} and {Dialogue} {Systems}: {A} {Hybrid} {Approach}},
  shorttitle = {A {Chatbot} {Dialogue} {Manager}-{Chatbots} and {Dialogue} {Systems}},
  school     = {Open Universiteit Nederland},
  author     = {Woudenberg, Aswin van},
  year       = {2014}
}

@book{jurafsky_speech_2009,
  title      = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition / {Daniel} {Jurafsky} ({Stanford} {University}), {James} {H}. {Martin} ({University} of {Colorado} at {Boulder})},
  isbn       = {978-0-13-504196-3},
  shorttitle = {Speech and language processing},
  language   = {eng},
  author     = {Jurafsky, Dan and Martin, James H. and Norvig, Peter and Russell, Stuart J.},
  year       = {2009},
  keywords   = {Automatische Spracherkennung, Computerlinguistik, Lehrbuch}
}


@book{lane_natural_2019,
  title      = {Natural language processing in action: understanding, analyzing, and generating text with {Python} / {Hobson} {Lane}, {Cole} {Howard}, {Hannes} {Max} {Hapke}},
  isbn       = {978-1-61729-463-1},
  shorttitle = {Natural language processing in action},
  language   = {eng},
  author     = {Lane, Hobson and Howard, Cole and Hapke, Hannes Max},
  year       = {2019},
  keywords   = {Automatische Sprachanalyse, Natürliche Sprache, Python (Programmiersprache), Textverstehendes System}
}

@misc{seq2seq_alammar,
  title  = {Sequence-to-Sequence Models for Chatbots},
  author = {Jay Alammar}
}

@misc{dialogflow_chawla,
  title  = {Building Chatbots with Google Dialogflow: Create chatbots with Dialogflow’s natural
            language processing and machine learning capabilities},
  author = {Sumit Chawla}
}

@misc{chatbot_development_sharma,
  title  = {Chatbot Development using Machine Learning Techniques},
  author = {Nupur Sharma, Anupriya}
}

@misc{noauthor_aiml_nodate,
  title   = {{AIML} {Foundation}},
  url     = {http://www.aiml.foundation/doc.html},
  urldate = {2023-03-12}
}

@book{diana_conversational_2011,
  title      = {Conversational {Agents} and {Natural} {Language} {Interaction}: {Techniques} and {Effective} {Practices}: {Techniques} and {Effective} {Practices}},
  isbn       = {978-1-60960-618-3},
  shorttitle = {Conversational {Agents} and {Natural} {Language} {Interaction}},
  abstract   = {By combining agent capabilities with computational linguistics, conversational agents can exploit natural language technologies to improve communication between humans and computers.Conversational Agents and Natural Language Interaction: Techniques and Effective Practices is a reference guide for researchers entering the promising field of conversational agents. It provides an introduction to fundamental concepts in the field, collects experiences of researchers working on conversational agents, and reviews techniques for the design and application of conversational agents. The book discusses the successes of and challenges faced by researchers, designers, and programmers who want to use conversational agents for e-commerce, help desks, website navigation, personalized service, and training or education applications.},
  language   = {en},
  publisher  = {IGI Global},
  author     = {Diana, Perez-Marin and Ismael, Pascual-Nieto},
  month      = jun,
  year       = {2011},
  note       = {Google-Books-ID: 2nUcqtbCOBcC},
  keywords   = {Computers / Computer Science, Computers / Cybernetics, Computers / Intelligence (AI) \& Semantics}
}

@misc{amazon_lex,
  author  = {amazon},
  title   = {Amazon {Lex}: {How} {It} {Works} - {Amazon} {Lex}},
  url     = {https://docs.aws.amazon.com/lex/latest/dg/how-it-works.html},
  urldate = {2023-03-12}
}

@misc{scikit-learn,
  author   = {{scikit-learn}},
  title    = {scikit-learn documentation — {DevDocs}},
  url      = {https://devdocs.io/scikit_learn/},
  abstract = {scikit-learn 1.1.3 API documentation with instant search, offline support, keyboard shortcuts, mobile version, and more.},
  language = {en},
  urldate  = {2023-03-12}
}

@misc{li_adversarial_2017,
  title     = {Adversarial {Learning} for {Neural} {Dialogue} {Generation}},
  url       = {http://arxiv.org/abs/1701.06547},
  doi       = {10.48550/arXiv.1701.06547},
  abstract  = {In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. In addition to adversarial training we describe a model for adversarial \{{\textbackslash}em evaluation\} that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.},
  urldate   = {2023-03-12},
  publisher = {arXiv},
  author    = {Li, Jiwei and Monroe, Will and Shi, Tianlin and Jean, Sébastien and Ritter, Alan and Jurafsky, Dan},
  month     = sep,
  year      = {2017},
  note      = {arXiv:1701.06547 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:C\:\\Users\\fgerv\\Zotero\\storage\\G42WFI22\\Li et al. - 2017 - Adversarial Learning for Neural Dialogue Generatio.pdf:application/pdf}
}

@misc{tensorflow,
  title   = {{TensorFlow} - {Sequence}-to-{Sequence} {Models}},
  url     = {https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.7/tensorflow/g3doc/tutorials/seq2seq/index.md},
  urldate = {2023-03-12},
  file    = {TensorFlow - Sequence-to-Sequence Models:C\:\\Users\\fgerv\\Zotero\\storage\\U2ASXMTK\\index.html:text/html},
  author  = {TensorFlow}
}

@book{marwedel_eingebettete_2021,
  address    = {Wiesbaden},
  title      = {Eingebettete {Systeme}: {Grundlagen} {Eingebetteter} {Systeme} in {Cyber}-{Physikalischen} {Systemen}},
  isbn       = {978-3-658-33436-9 978-3-658-33437-6},
  shorttitle = {Eingebettete {Systeme}},
  url        = {https://link.springer.com/10.1007/978-3-658-33437-6},
  language   = {de},
  urldate    = {2023-03-15},
  publisher  = {Springer Fachmedien},
  author     = {Marwedel, Peter},
  year       = {2021},
  doi        = {10.1007/978-3-658-33437-6},
  keywords   = {Betriebssystem, Cyber-Physikalische Systeme, Echtzeitbetriebssysteme, Eingebettete Systeme Buch, Hardware/Software-Codesign, Internet der Dinge, Internet of Things, Open Access, Scheduling, Spezifikationssprachen},
  file       = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\FIFMZ4PP\\Marwedel - 2021 - Eingebettete Systeme Grundlagen Eingebetteter Sys.pdf:application/pdf}
}

@article{jansen_digitalisierung_2017,
  title      = {Digitalisierung: 8,4 {Milliarden} vernetzte {Geräte} im {Internet} der {Dinge}},
  issn       = {0174-4909},
  shorttitle = {Digitalisierung},
  url        = {https://www.faz.net/aktuell/wirtschaft/netzwirtschaft/digitalisierung-8-4-milliarden-vernetzte-geraete-im-internet-der-dinge-14865654.html},
  abstract   = {Kaffeemaschinen, Kameras, Babyphones: Die Zahl der Haushaltsgeräte, die mit dem Internet verbunden sind, wächst rasant. Experten machen nun eine gewaltige Prognose.},
  language   = {de},
  urldate    = {2023-03-15},
  journal    = {FAZ.NET},
  author     = {Jansen, Jonas},
  month      = feb,
  year       = {2017},
  keywords   = {Digitalisierung, Internet der Dinge, Marktforschungsgesellschaft, Sensor, Vernetzung}
}

@misc{ltd_raspberry_nodate,
  title    = {Raspberry {Pi} 4 {Model} {B} specifications},
  url      = {https://www.raspberrypi.com/products/raspberry-pi-4-model-b/specifications/},
  abstract = {Your tiny, dual-display, desktop computer
              …and robot brains, smart home hub, media centre, networked AI core, factory controller, and much more.},
  language = {en-GB},
  urldate  = {2023-03-24},
  journal  = {Raspberry Pi},
  author   = {{Ltd, Raspberry Pi}}
}

@misc{mischmaschine,
  author       = {Felix Götz and Moritz Höckele and Florian Lobert},
  howpublished = {Projektarbeit Software des Studiengangs Mechatronik an der DHBW Karlsruhe},
  title        = {Mischmaschine},
  year         = {2022}
}

@incollection{herczeg_9_2018,
  title     = {9. {Zeitverhalten} interaktiver {Systeme}},
  isbn      = {978-3-11-044686-9},
  url       = {https://www.degruyter.com/document/doi/10.1515/9783110446869-187/html},
  abstract  = {9. Zeitverhalten interaktiver Systeme was published in Software-Ergonomie on page 173.},
  language  = {de},
  urldate   = {2023-03-26},
  booktitle = {9. {Zeitverhalten} interaktiver {Systeme}},
  publisher = {De Gruyter Oldenbourg},
  author    = {Herczeg, Michael},
  month     = mar,
  year      = {2018},
  doi       = {10.1515/9783110446869-187},
  pages     = {173--182},
  file      = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\5Q55E87B\\Herczeg - 2018 - 9. Zeitverhalten interaktiver Systeme.pdf:application/pdf}
}

@misc{google_cloud_speech,
  title      = {Speech-to-{Text}: {Automatic} {Speech} {Recognition}},
  shorttitle = {Speech-to-{Text}},
  url        = {https://cloud.google.com/speech-to-text},
  abstract   = {Accurately convert voice to text in over 125 languages and variants by applying Google’s powerful machine learning models with an easy-to-use API.},
  language   = {en},
  urldate    = {2023-03-26},
  journal    = {Google Cloud},
  file       = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\CI9HZA8I\\speech-to-text.html:text/html},
  author     = {Google}
}

@misc{speechrecognition,
  title      = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
  copyright  = {BSD License},
  shorttitle = {{SpeechRecognition}},
  url        = {https://github.com/Uberi/speech_recognition#readme},
  urldate    = {2023-03-30},
  author     = {Zhang (Uberi), Anthony},
  keywords   = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
  file       = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\ANLS7AR6\\SpeechRecognition.html:text/html}
}

@misc{openai,
  title    = {About},
  url      = {https://openai.com/about},
  abstract = {OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.},
  language = {en-US},
  urldate  = {2023-04-01},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\3UUKF6ZX\\about.html:text/html},
  author   = {OpenAI}
}

@misc{whisper,
  title    = {Introducing {Whisper}},
  url      = {https://openai.com/research/whisper},
  abstract = {We’ve trained and are open-sourcing a neural net called Whisper that approaches human level robustness and accuracy on English speech recognition.},
  language = {en-US},
  urldate  = {2023-04-01},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\5MB7DV8W\\whisper.html:text/html},
  author   = {OpenAI}
}

@misc{witai,
  title   = {Wit.ai},
  url     = {https://wit.ai/},
  urldate = {2023-04-02},
  file    = {Wit.ai:C\:\\Users\\fgerv\\Zotero\\storage\\97R95F4X\\wit.ai.html:text/html},
  author  = {Wit.ai}
}

@misc{tensorflow_home,
  title    = {{TensorFlow}},
  url      = {https://www.tensorflow.org/},
  abstract = {An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources.},
  language = {en},
  urldate  = {2023-04-02},
  journal  = {TensorFlow},
  author   = {TensorFlow}
}

@misc{houndify,
  title    = {Home},
  url      = {https://www.soundhound.com/},
  abstract = {Voice AI interfaces for hardware devices, services, vehicles, mobile apps, and more powered by SoundHound's conversational intelligence solutions},
  language = {en-US},
  urldate  = {2023-04-02},
  journal  = {SoundHound},
  author   = {SoundHound}
}

@misc{azure_speech_to_text,
  title    = {Speech to {Text} – {Audio} to {Text} {Translation} {\textbar} {Microsoft} {Azure}},
  url      = {https://azure.microsoft.com/en-us/products/cognitive-services/speech-to-text},
  abstract = {Get Microsoft Purview, a unified data governance service for data management. Manage and govern your on-premises, multicloud, and software-as-a-service (SaaS) data.},
  language = {en-US},
  urldate  = {2023-04-02},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\LM7FGI7D\\speech-to-text.html:text/html},
  author   = {Microsoft}
}

@misc{ibm_speech_to_text,
  title    = {{IBM} {Watson} {Speech} to {Text} - {Overview}},
  url      = {https://www.ibm.com/cloud/watson-speech-to-text},
  abstract = {IBM Watson Speech to Text (STT) is a service on the IBM Cloud that enables you to easily convert audio and voice into written text.},
  language = {en-us},
  urldate  = {2023-04-02},
  month    = mar,
  year     = {2023},
  author   = {IBM}
}

@misc{sphinx_about,
  title    = {About {CMUSphinx}},
  url      = {http://cmusphinx.github.io/wiki/about/},
  abstract = {CMUSphinx is an open source speech recognition system for mobile and server applications. Supported languages: C, C++, C\#, Python, Ruby, Java, Javascript. Supported platforms: Unix, Windows, IOS, Android, hardware.},
  urldate  = {2023-04-02},
  journal  = {CMUSphinx Open Source Speech Recognition},
  author   = {Shmyrev, Nickolay},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\FPVQMD2J\\about.html:text/html}
}

@misc{whisper_repo,
  title     = {Whisper},
  copyright = {MIT},
  url       = {https://github.com/openai/whisper},
  abstract  = {Robust Speech Recognition via Large-Scale Weak Supervision},
  urldate   = {2023-04-02},
  publisher = {OpenAI},
  month     = apr,
  year      = {2023},
  note      = {original-date: 2022-09-16T20:02:54Z},
  author    = {OpenAI}
}

@book{goldberg_neural_2017,
  address   = {San Rafael, Calif.},
  title     = {Neural {Network} {Methods} in {Natural} {Language} {Processing}},
  isbn      = {978-1-62705-298-6},
  abstract  = {Neural networks are a family of powerful machine learning models and this book focuses on their application to natural language data.The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries.The second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning.},
  language  = {Englisch},
  publisher = {Morgan \& Claypool Publishers},
  author    = {Goldberg, Yoav},
  editor    = {Hirst, Graeme},
  month     = apr,
  year      = {2017}
}

@misc{snowboy,
  title     = {Snowboy {Hotword} {Detection}},
  url       = {https://github.com/Kitt-AI/snowboy},
  abstract  = {Future versions with model training module will be maintained through a forked version here: https://github.com/seasalt-ai/snowboy},
  urldate   = {2023-04-03},
  publisher = {Kitt-AI},
  month     = apr,
  year      = {2023},
  note      = {original-date: 2016-05-10T00:54:30Z},
  author    = {Kitt-AI}
}

@misc{vosk_repo,
  title     = {Vosk {Speech} {Recognition} {Toolkit}},
  copyright = {Apache-2.0},
  url       = {https://github.com/alphacep/vosk-api},
  abstract  = {Offline speech recognition API for Android, iOS, Raspberry Pi and servers with Python, Java, C\# and Node},
  urldate   = {2023-04-03},
  publisher = {Alpha Cephei},
  month     = apr,
  year      = {2023},
  note      = {original-date: 2019-09-03T17:48:42Z},
  keywords  = {android, asr, deep-learning, deep-neural-networks, deepspeech, google-speech-to-text, ios, kaldi, offline, privacy, python, raspberry-pi, speaker-identification, speaker-verification, speech-recognition, speech-to-text, speech-to-text-android, stt, voice-recognition, vosk},
  author    = {{Alpha Cephei}}
}

@misc{vosk,
  title    = {{VOSK} {Offline} {Speech} {Recognition} {API}},
  url      = {https://alphacephei.com/vosk/},
  abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
  language = {en},
  urldate  = {2023-04-03},
  journal  = {VOSK Offline Speech Recognition API},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\5KIY7EZQ\\vosk.html:text/html},
  author   = {{Alpha Cephei}}
}

@misc{pytorch,
  title    = {{PyTorch}},
  url      = {https://www.pytorch.org},
  abstract = {An open source machine learning framework that accelerates the path from research prototyping to production deployment.},
  language = {en},
  urldate  = {2023-04-03},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\YV87B93A\\pytorch.org.html:text/html},
  author   = {PyTorch}
}

@misc{pip,
  title      = {pip: {The} {PyPA} recommended tool for installing {Python} packages.},
  copyright  = {MIT License},
  shorttitle = {pip},
  url        = {https://pip.pypa.io/},
  urldate    = {2023-04-03},
  author     = {Python Packaging Authority PPA},
  keywords   = {Software Development - Build Tools}
}

@misc{q-engineering_install_nodate,
  title    = {Install {PyTorch} on {Raspberry} {Pi} 4 - {Q}-engineering},
  url      = {https://qengineering.eu/install-pytorch-on-raspberry-pi-4.html},
  abstract = {A thorough guide on how to install PyTorch on the Raspberry Pi 4},
  language = {en-GB},
  urldate  = {2023-04-04},
  author   = {Q-engineering},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\JINPGI7Z\\install-pytorch-on-raspberry-pi-4.html:text/html}
}

@misc{pyserial,
  title   = {Welcome to {pySerial}’s documentation — {pySerial} 3.0 documentation},
  url     = {https://pythonhosted.org/pyserial/},
  urldate = {2023-04-04},
  file    = {Welcome to pySerial’s documentation — pySerial 3.0 documentation:C\:\\Users\\fgerv\\Zotero\\storage\\Y24I7G68\\pyserial.html:text/html},
  author  = {PySerial}
}

@misc{rygl_semantic_2017,
  title     = {Semantic {Vector} {Encoding} and {Similarity} {Search} {Using} {Fulltext} {Search} {Engines}},
  url       = {http://arxiv.org/abs/1706.00957},
  abstract  = {Vector representations and vector space modeling (VSM) play a central role in modern machine learning. We propose a novel approach to `vector similarity searching' over dense semantic representations of words and documents that can be deployed on top of traditional inverted-index-based fulltext engines, taking advantage of their robustness, stability, scalability and ubiquity. We show that this approach allows the indexing and querying of dense vectors in text domains. This opens up exciting avenues for major efficiency gains, along with simpler deployment, scaling and monitoring. The end result is a fast and scalable vector database with a tunable trade-off between vector search performance and quality, backed by a standard fulltext engine such as Elasticsearch. We empirically demonstrate its querying performance and quality by applying this solution to the task of semantic searching over a dense vector representation of the entire English Wikipedia.},
  urldate   = {2023-04-06},
  publisher = {arXiv},
  author    = {Rygl, Jan and Pomikálek, Jan and Řehůřek, Radim and Růžička, Michal and Novotný, Vít and Sojka, Petr},
  month     = jun,
  year      = {2017},
  note      = {arXiv:1706.00957 [cs]},
  keywords  = {Computer Science - Information Retrieval},
  file      = {arXiv Fulltext PDF:C\:\\Users\\fgerv\\Zotero\\storage\\WZG579BA\\Rygl et al. - 2017 - Semantic Vector Encoding and Similarity Search Usi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\PWARDLQB\\1706.html:text/html}
}

@article{eckart_approximation_1936,
  title    = {The approximation of one matrix by another of lower rank},
  volume   = {1},
  issn     = {1860-0980},
  url      = {https://doi.org/10.1007/BF02288367},
  doi      = {10.1007/BF02288367},
  abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.},
  language = {en},
  number   = {3},
  urldate  = {2023-04-06},
  journal  = {Psychometrika},
  author   = {Eckart, Carl and Young, Gale},
  month    = sep,
  year     = {1936},
  keywords = {Canonic Form, Lower Rank, Mathematical Problem, Public Policy, Statistical Theory},
  pages    = {211--218},
  file     = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\9HICRCED\\Eckart and Young - 1936 - The approximation of one matrix by another of lowe.pdf:application/pdf}
}

@inproceedings{wong_generalized_1985,
  address   = {New York, NY, USA},
  series    = {{SIGIR} '85},
  title     = {Generalized vector spaces model in information retrieval},
  isbn      = {978-0-89791-159-7},
  url       = {https://dl.acm.org/doi/10.1145/253495.253506},
  doi       = {10.1145/253495.253506},
  abstract  = {In information retrieval, it is common to model index terms and documents as vectors in a suitably defined vector space. The main difficulty with this approach is that the explicit representation of term vectors is not known a priori. For this reason, the vector space model adopted by Salton for the SMART system treats the terms as a set of orthogonal vectors. In such a model it is often necessary to adopt a separate, corrective procedure to take into account the correlations between terms. In this paper, we propose a systematic method (the generalized vector space model) to compute term correlations directly from automatic indexing scheme. We also demonstrate how such correlations can be included with minimal modification in the existing vector based information retrieval systems. The preliminary experimental results obtained from the new model are very encouraging.},
  urldate   = {2023-04-06},
  booktitle = {Proceedings of the 8th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
  publisher = {Association for Computing Machinery},
  author    = {Wong, S. K. M. and Ziarko, Wojciech and Wong, Patrick C. N.},
  month     = jun,
  year      = {1985},
  pages     = {18--25},
  file      = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\JMUXZIKK\\Wong et al. - 1985 - Generalized vector spaces model in information ret.pdf:application/pdf}
}

@inproceedings{isbell_restructuring_1998,
  title     = {Restructuring {Sparse} {High} {Dimensional} {Data} for {Effective} {Retrieval}},
  volume    = {11},
  url       = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/87ec2f451208df97228105657edb717f-Abstract.html},
  abstract  = {The task in text retrieval is to find  the subset of a collection of documents relevant  to  a user's  information request,  usually expressed as  a set of words.  Classically,  documents and queries are represented as  vectors of word counts.  In  its  simplest  form, relevance is  defined to be the dot product between a document and  a query  vector-a measure  of the  number of common  terms.  A  central  difficulty  in  text  retrieval  is  that  the  presence or absence  of a  word  is  not sufficient to  determine  relevance to a query. Linear dimensionality reduction has been proposed as a tech(cid:173) nique for extracting underlying structure from  the document collection.  In  some  domains  (such  as  vision)  dimensionality  reduction  reduces  computational com(cid:173) plexity.  In  text retrieval  it is  more often  used  to  improve retrieval  performance.  We  propose  an  alternative  and  novel  technique  that  produces  sparse  represen(cid:173) tations  constructed  from  sets  of highly-related  words.  Documents  and  queries  are  represented  by  their distance  to  these  sets,  and relevance  is  measured by  the  number of common clusters.  This technique significantly improves retrieval  per(cid:173) formance,  is  efficient  to  compute  and  shares  properties  with  the  optimal  linear  projection operator and the independent components of documents.},
  urldate   = {2023-04-06},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {MIT Press},
  author    = {Isbell, Charles and Viola, Paul},
  year      = {1998},
  file      = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\DSSW5H6B\\Isbell and Viola - 1998 - Restructuring Sparse High Dimensional Data for Eff.pdf:application/pdf}
}

@misc{koech_cross-entropy_2022,
  title    = {Cross-{Entropy} {Loss} {Function}},
  url      = {https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e},
  abstract = {A loss function used in most classification problems to optimize machine learning model…},
  language = {en},
  urldate  = {2023-04-06},
  journal  = {Medium},
  author   = {Koech, Kiprono Elijah},
  month    = jul,
  year     = {2022}
}

@misc{arduino_mega,
  title    = {Arduino {Mega} 2560 {Rev3}},
  url      = {https://store.arduino.cc/products/arduino-mega-2560-rev3},
  abstract = {The Arduino Mega 2560 is a microcontroller board based on the ATmega2560. It has 54 digital input/output pins (of which 15 can be used as PWM outputs), 16 analog inputs, 4 UARTs (hardware serial ports), a 16 MHz crystal oscillator, a USB connection, a power jack, an ICSP header, and a reset button. It contains everythi},
  language = {en},
  urldate  = {2023-04-07},
  journal  = {Arduino Official Store},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\93RB5BBW\\arduino-mega-2560-rev3.html:text/html},
  author   = {Arduino}
}

@misc{pyttsx3,
  title   = {pyttsx3 - {Text}-to-speech x-platform — pyttsx3 2.6 documentation},
  url     = {https://pyttsx3.readthedocs.io/en/latest/index.html},
  urldate = {2023-04-07},
  file    = {pyttsx3 - Text-to-speech x-platform — pyttsx3 2.6 documentation:C\:\\Users\\fgerv\\Zotero\\storage\\4TBHKUYW\\index.html:text/html},
  author  = {pyttsx3}
}

@article{deerwester_indexing_1990,
  title    = {Indexing by latent semantic analysis},
  volume   = {41},
  issn     = {1097-4571},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
  doi      = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
  abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
  language = {en},
  number   = {6},
  urldate  = {2023-04-06},
  journal  = {Journal of the American Society for Information Science},
  author   = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  year     = {1990},
  note     = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-4571\%28199009\%2941\%3A6\%3C391\%3A\%3AAID-ASI1\%3E3.0.CO\%3B2-9},
  pages    = {391--407},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\RMW6UW57\\(SICI)1097-4571(199009)416391AID-ASI13.0.html:text/html;Submitted Version:C\:\\Users\\fgerv\\Zotero\\storage\\JAUBXCZZ\\Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:application/pdf}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
