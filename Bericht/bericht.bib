@mastersthesis{woudenberg_chatbot_2014,
  title      = {A {Chatbot} {Dialogue} {Manager}-{Chatbots} and {Dialogue} {Systems}: {A} {Hybrid} {Approach}},
  shorttitle = {A {Chatbot} {Dialogue} {Manager}-{Chatbots} and {Dialogue} {Systems}},
  school     = {Open Universiteit Nederland},
  author     = {Woudenberg, Aswin van},
  year       = {2014}
}

@book{jurafsky_speech_2009,
  title      = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition / {Daniel} {Jurafsky} ({Stanford} {University}), {James} {H}. {Martin} ({University} of {Colorado} at {Boulder})},
  isbn       = {978-0-13-504196-3},
  shorttitle = {Speech and language processing},
  language   = {eng},
  author     = {Jurafsky, Dan and Martin, James H. and Norvig, Peter and Russell, Stuart J.},
  year       = {2009},
  keywords   = {Automatische Spracherkennung, Computerlinguistik, Lehrbuch}
}


@book{lane_natural_2019,
  title      = {Natural language processing in action: understanding, analyzing, and generating text with {Python} / {Hobson} {Lane}, {Cole} {Howard}, {Hannes} {Max} {Hapke}},
  isbn       = {978-1-61729-463-1},
  shorttitle = {Natural language processing in action},
  language   = {eng},
  author     = {Lane, Hobson and Howard, Cole and Hapke, Hannes Max},
  year       = {2019},
  keywords   = {Automatische Sprachanalyse, Natürliche Sprache, Python (Programmiersprache), Textverstehendes System}
}

@misc{seq2seq_alammar,
  title  = {Sequence-to-Sequence Models for Chatbots},
  author = {Jay Alammar}
}

@misc{dialogflow_chawla,
  title  = {Building Chatbots with Google Dialogflow: Create chatbots with Dialogflow’s natural
            language processing and machine learning capabilities},
  author = {Sumit Chawla}
}

@misc{chatbot_development_sharma,
  title  = {Chatbot Development using Machine Learning Techniques},
  author = {Nupur Sharma, Anupriya}
}

@misc{noauthor_aiml_nodate,
  title   = {{AIML} {Foundation}},
  url     = {http://www.aiml.foundation/doc.html},
  urldate = {2023-03-12}
}

@book{diana_conversational_2011,
  title      = {Conversational {Agents} and {Natural} {Language} {Interaction}: {Techniques} and {Effective} {Practices}: {Techniques} and {Effective} {Practices}},
  isbn       = {978-1-60960-618-3},
  shorttitle = {Conversational {Agents} and {Natural} {Language} {Interaction}},
  abstract   = {By combining agent capabilities with computational linguistics, conversational agents can exploit natural language technologies to improve communication between humans and computers.Conversational Agents and Natural Language Interaction: Techniques and Effective Practices is a reference guide for researchers entering the promising field of conversational agents. It provides an introduction to fundamental concepts in the field, collects experiences of researchers working on conversational agents, and reviews techniques for the design and application of conversational agents. The book discusses the successes of and challenges faced by researchers, designers, and programmers who want to use conversational agents for e-commerce, help desks, website navigation, personalized service, and training or education applications.},
  language   = {en},
  publisher  = {IGI Global},
  author     = {Diana, Perez-Marin and Ismael, Pascual-Nieto},
  month      = jun,
  year       = {2011},
  note       = {Google-Books-ID: 2nUcqtbCOBcC},
  keywords   = {Computers / Computer Science, Computers / Cybernetics, Computers / Intelligence (AI) \& Semantics}
}

@misc{amazon_lex,
  author  = {amazon},
  title   = {Amazon {Lex}: {How} {It} {Works} - {Amazon} {Lex}},
  url     = {https://docs.aws.amazon.com/lex/latest/dg/how-it-works.html},
  urldate = {2023-03-12}
}

@misc{scikit-learn,
  author   = {{scikit-learn}},
  title    = {scikit-learn documentation — {DevDocs}},
  url      = {https://devdocs.io/scikit_learn/},
  abstract = {scikit-learn 1.1.3 API documentation with instant search, offline support, keyboard shortcuts, mobile version, and more.},
  language = {en},
  urldate  = {2023-03-12}
}

@misc{li_adversarial_2017,
  title     = {Adversarial {Learning} for {Neural} {Dialogue} {Generation}},
  url       = {http://arxiv.org/abs/1701.06547},
  doi       = {10.48550/arXiv.1701.06547},
  abstract  = {In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. In addition to adversarial training we describe a model for adversarial \{{\textbackslash}em evaluation\} that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.},
  urldate   = {2023-03-12},
  publisher = {arXiv},
  author    = {Li, Jiwei and Monroe, Will and Shi, Tianlin and Jean, Sébastien and Ritter, Alan and Jurafsky, Dan},
  month     = sep,
  year      = {2017},
  note      = {arXiv:1701.06547 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:C\:\\Users\\fgerv\\Zotero\\storage\\G42WFI22\\Li et al. - 2017 - Adversarial Learning for Neural Dialogue Generatio.pdf:application/pdf}
}

@misc{tensorflow,
  title   = {{TensorFlow} - {Sequence}-to-{Sequence} {Models}},
  url     = {https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.7/tensorflow/g3doc/tutorials/seq2seq/index.md},
  urldate = {2023-03-12},
  file    = {TensorFlow - Sequence-to-Sequence Models:C\:\\Users\\fgerv\\Zotero\\storage\\U2ASXMTK\\index.html:text/html},
  author  = {TensorFlow}
}

@book{marwedel_eingebettete_2021,
  address    = {Wiesbaden},
  title      = {Eingebettete {Systeme}: {Grundlagen} {Eingebetteter} {Systeme} in {Cyber}-{Physikalischen} {Systemen}},
  isbn       = {978-3-658-33436-9 978-3-658-33437-6},
  shorttitle = {Eingebettete {Systeme}},
  url        = {https://link.springer.com/10.1007/978-3-658-33437-6},
  language   = {de},
  urldate    = {2023-03-15},
  publisher  = {Springer Fachmedien},
  author     = {Marwedel, Peter},
  year       = {2021},
  doi        = {10.1007/978-3-658-33437-6},
  keywords   = {Betriebssystem, Cyber-Physikalische Systeme, Echtzeitbetriebssysteme, Eingebettete Systeme Buch, Hardware/Software-Codesign, Internet der Dinge, Internet of Things, Open Access, Scheduling, Spezifikationssprachen},
  file       = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\FIFMZ4PP\\Marwedel - 2021 - Eingebettete Systeme Grundlagen Eingebetteter Sys.pdf:application/pdf}
}

@article{jansen_digitalisierung_2017,
  title      = {Digitalisierung: 8,4 {Milliarden} vernetzte {Geräte} im {Internet} der {Dinge}},
  issn       = {0174-4909},
  shorttitle = {Digitalisierung},
  url        = {https://www.faz.net/aktuell/wirtschaft/netzwirtschaft/digitalisierung-8-4-milliarden-vernetzte-geraete-im-internet-der-dinge-14865654.html},
  abstract   = {Kaffeemaschinen, Kameras, Babyphones: Die Zahl der Haushaltsgeräte, die mit dem Internet verbunden sind, wächst rasant. Experten machen nun eine gewaltige Prognose.},
  language   = {de},
  urldate    = {2023-03-15},
  journal    = {FAZ.NET},
  author     = {Jansen, Jonas},
  month      = feb,
  year       = {2017},
  keywords   = {Digitalisierung, Internet der Dinge, Marktforschungsgesellschaft, Sensor, Vernetzung}
}

@misc{ltd_raspberry_nodate,
  title    = {Raspberry {Pi} 4 {Model} {B} specifications},
  url      = {https://www.raspberrypi.com/products/raspberry-pi-4-model-b/specifications/},
  abstract = {Your tiny, dual-display, desktop computer
              …and robot brains, smart home hub, media centre, networked AI core, factory controller, and much more.},
  language = {en-GB},
  urldate  = {2023-03-24},
  journal  = {Raspberry Pi},
  author   = {{Ltd, Raspberry Pi}}
}

@misc{mischmaschine,
  author       = {Felix Götz and Moritz Höckele and Florian Lobert},
  howpublished = {Projektarbeit Software des Studiengangs Mechatronik an der DHBW Karlsruhe},
  title        = {Mischmaschine},
  year         = {2022}
}

@incollection{herczeg_9_2018,
  title     = {9. {Zeitverhalten} interaktiver {Systeme}},
  isbn      = {978-3-11-044686-9},
  url       = {https://www.degruyter.com/document/doi/10.1515/9783110446869-187/html},
  abstract  = {9. Zeitverhalten interaktiver Systeme was published in Software-Ergonomie on page 173.},
  language  = {de},
  urldate   = {2023-03-26},
  booktitle = {9. {Zeitverhalten} interaktiver {Systeme}},
  publisher = {De Gruyter Oldenbourg},
  author    = {Herczeg, Michael},
  month     = mar,
  year      = {2018},
  doi       = {10.1515/9783110446869-187},
  pages     = {173--182},
  file      = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\5Q55E87B\\Herczeg - 2018 - 9. Zeitverhalten interaktiver Systeme.pdf:application/pdf}
}

@misc{google_cloud_speech,
  title      = {Speech-to-{Text}: {Automatic} {Speech} {Recognition}},
  shorttitle = {Speech-to-{Text}},
  url        = {https://cloud.google.com/speech-to-text},
  abstract   = {Accurately convert voice to text in over 125 languages and variants by applying Google’s powerful machine learning models with an easy-to-use API.},
  language   = {en},
  urldate    = {2023-03-26},
  journal    = {Google Cloud},
  file       = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\CI9HZA8I\\speech-to-text.html:text/html},
  author     = {Google}
}

@misc{speechrecognition,
  title      = {{SpeechRecognition}: {Library} for performing speech recognition, with support for several engines and {APIs}, online and offline.},
  copyright  = {BSD License},
  shorttitle = {{SpeechRecognition}},
  url        = {https://github.com/Uberi/speech_recognition#readme},
  urldate    = {2023-03-30},
  author     = {Zhang (Uberi), Anthony},
  keywords   = {api,, bing,, google,, houndify,, ibm,, Multimedia - Sound/Audio - Speech, recognition,, snowboy, Software Development - Libraries - Python Modules, speech,, sphinx,, voice,, wit,},
  file       = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\ANLS7AR6\\SpeechRecognition.html:text/html}
}

@misc{openai,
  title    = {About},
  url      = {https://openai.com/about},
  abstract = {OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.},
  language = {en-US},
  urldate  = {2023-04-01},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\3UUKF6ZX\\about.html:text/html},
  author   = {OpenAI}
}

@misc{whisper,
  title    = {Introducing {Whisper}},
  url      = {https://openai.com/research/whisper},
  abstract = {We’ve trained and are open-sourcing a neural net called Whisper that approaches human level robustness and accuracy on English speech recognition.},
  language = {en-US},
  urldate  = {2023-04-01},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\5MB7DV8W\\whisper.html:text/html},
  author   = {OpenAI}
}

@misc{witai,
  title   = {Wit.ai},
  url     = {https://wit.ai/},
  urldate = {2023-04-02},
  file    = {Wit.ai:C\:\\Users\\fgerv\\Zotero\\storage\\97R95F4X\\wit.ai.html:text/html},
  author  = {Wit.ai}
}

@misc{tensorflow_home,
  title    = {{TensorFlow}},
  url      = {https://www.tensorflow.org/},
  abstract = {An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources.},
  language = {en},
  urldate  = {2023-04-02},
  journal  = {TensorFlow},
  author   = {TensorFlow}
}

@misc{houndify,
  title    = {Home},
  url      = {https://www.soundhound.com/},
  abstract = {Voice AI interfaces for hardware devices, services, vehicles, mobile apps, and more powered by SoundHound's conversational intelligence solutions},
  language = {en-US},
  urldate  = {2023-04-02},
  journal  = {SoundHound},
  author   = {SoundHound}
}

@misc{azure_speech_to_text,
  title    = {Speech to {Text} – {Audio} to {Text} {Translation} {\textbar} {Microsoft} {Azure}},
  url      = {https://azure.microsoft.com/en-us/products/cognitive-services/speech-to-text},
  abstract = {Get Microsoft Purview, a unified data governance service for data management. Manage and govern your on-premises, multicloud, and software-as-a-service (SaaS) data.},
  language = {en-US},
  urldate  = {2023-04-02},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\LM7FGI7D\\speech-to-text.html:text/html},
  author   = {Microsoft}
}

@misc{ibm_speech_to_text,
  title    = {{IBM} {Watson} {Speech} to {Text} - {Overview}},
  url      = {https://www.ibm.com/cloud/watson-speech-to-text},
  abstract = {IBM Watson Speech to Text (STT) is a service on the IBM Cloud that enables you to easily convert audio and voice into written text.},
  language = {en-us},
  urldate  = {2023-04-02},
  month    = mar,
  year     = {2023},
  author   = {IBM}
}

@misc{sphinx_about,
  title    = {About {CMUSphinx}},
  url      = {http://cmusphinx.github.io/wiki/about/},
  abstract = {CMUSphinx is an open source speech recognition system for mobile and server applications. Supported languages: C, C++, C\#, Python, Ruby, Java, Javascript. Supported platforms: Unix, Windows, IOS, Android, hardware.},
  urldate  = {2023-04-02},
  journal  = {CMUSphinx Open Source Speech Recognition},
  author   = {Shmyrev, Nickolay},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\FPVQMD2J\\about.html:text/html}
}

@misc{whisper_repo,
  title     = {Whisper},
  copyright = {MIT},
  url       = {https://github.com/openai/whisper},
  abstract  = {Robust Speech Recognition via Large-Scale Weak Supervision},
  urldate   = {2023-04-02},
  publisher = {OpenAI},
  month     = apr,
  year      = {2023},
  note      = {original-date: 2022-09-16T20:02:54Z},
  author    = {OpenAI}
}

@book{goldberg_neural_2017,
  address   = {San Rafael, Calif.},
  title     = {Neural {Network} {Methods} in {Natural} {Language} {Processing}},
  isbn      = {978-1-62705-298-6},
  abstract  = {Neural networks are a family of powerful machine learning models and this book focuses on their application to natural language data.The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries.The second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning.},
  language  = {Englisch},
  publisher = {Morgan \& Claypool Publishers},
  author    = {Goldberg, Yoav},
  editor    = {Hirst, Graeme},
  month     = apr,
  year      = {2017}
}

@misc{snowboy,
  title     = {Snowboy {Hotword} {Detection}},
  url       = {https://github.com/Kitt-AI/snowboy},
  abstract  = {Future versions with model training module will be maintained through a forked version here: https://github.com/seasalt-ai/snowboy},
  urldate   = {2023-04-03},
  publisher = {Kitt-AI},
  month     = apr,
  year      = {2023},
  note      = {original-date: 2016-05-10T00:54:30Z},
  author    = {Kitt-AI}
}

@misc{vosk_repo,
  title     = {Vosk {Speech} {Recognition} {Toolkit}},
  copyright = {Apache-2.0},
  url       = {https://github.com/alphacep/vosk-api},
  abstract  = {Offline speech recognition API for Android, iOS, Raspberry Pi and servers with Python, Java, C\# and Node},
  urldate   = {2023-04-03},
  publisher = {Alpha Cephei},
  month     = apr,
  year      = {2023},
  note      = {original-date: 2019-09-03T17:48:42Z},
  keywords  = {android, asr, deep-learning, deep-neural-networks, deepspeech, google-speech-to-text, ios, kaldi, offline, privacy, python, raspberry-pi, speaker-identification, speaker-verification, speech-recognition, speech-to-text, speech-to-text-android, stt, voice-recognition, vosk},
  author    = {{Alpha Cephei}}
}

@misc{vosk,
  title    = {{VOSK} {Offline} {Speech} {Recognition} {API}},
  url      = {https://alphacephei.com/vosk/},
  abstract = {Accurate speech recognition for Android, iOS, Raspberry Pi and servers with Python, Java, C\#, Swift and Node.},
  language = {en},
  urldate  = {2023-04-03},
  journal  = {VOSK Offline Speech Recognition API},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\5KIY7EZQ\\vosk.html:text/html},
  author   = {{Alpha Cephei}}
}

@misc{pytorch,
  title    = {{PyTorch}},
  url      = {https://www.pytorch.org},
  abstract = {An open source machine learning framework that accelerates the path from research prototyping to production deployment.},
  language = {en},
  urldate  = {2023-04-03},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\YV87B93A\\pytorch.org.html:text/html},
  author   = {PyTorch}
}

@misc{pip,
  title      = {pip: {The} {PyPA} recommended tool for installing {Python} packages.},
  copyright  = {MIT License},
  shorttitle = {pip},
  url        = {https://pip.pypa.io/},
  urldate    = {2023-04-03},
  author     = {Python Packaging Authority PPA},
  keywords   = {Software Development - Build Tools}
}

@misc{q-engineering_install_nodate,
  title    = {Install {PyTorch} on {Raspberry} {Pi} 4 - {Q}-engineering},
  url      = {https://qengineering.eu/install-pytorch-on-raspberry-pi-4.html},
  abstract = {A thorough guide on how to install PyTorch on the Raspberry Pi 4},
  language = {en-GB},
  urldate  = {2023-04-04},
  author   = {Q-engineering},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\JINPGI7Z\\install-pytorch-on-raspberry-pi-4.html:text/html}
}

@misc{pyserial,
  title   = {Welcome to {pySerial}’s documentation — {pySerial} 3.0 documentation},
  url     = {https://pythonhosted.org/pyserial/},
  urldate = {2023-04-04},
  file    = {Welcome to pySerial’s documentation — pySerial 3.0 documentation:C\:\\Users\\fgerv\\Zotero\\storage\\Y24I7G68\\pyserial.html:text/html},
  author  = {PySerial}
}

@misc{rygl_semantic_2017,
  title     = {Semantic {Vector} {Encoding} and {Similarity} {Search} {Using} {Fulltext} {Search} {Engines}},
  url       = {http://arxiv.org/abs/1706.00957},
  abstract  = {Vector representations and vector space modeling (VSM) play a central role in modern machine learning. We propose a novel approach to `vector similarity searching' over dense semantic representations of words and documents that can be deployed on top of traditional inverted-index-based fulltext engines, taking advantage of their robustness, stability, scalability and ubiquity. We show that this approach allows the indexing and querying of dense vectors in text domains. This opens up exciting avenues for major efficiency gains, along with simpler deployment, scaling and monitoring. The end result is a fast and scalable vector database with a tunable trade-off between vector search performance and quality, backed by a standard fulltext engine such as Elasticsearch. We empirically demonstrate its querying performance and quality by applying this solution to the task of semantic searching over a dense vector representation of the entire English Wikipedia.},
  urldate   = {2023-04-06},
  publisher = {arXiv},
  author    = {Rygl, Jan and Pomikálek, Jan and Řehůřek, Radim and Růžička, Michal and Novotný, Vít and Sojka, Petr},
  month     = jun,
  year      = {2017},
  note      = {arXiv:1706.00957 [cs]},
  keywords  = {Computer Science - Information Retrieval},
  file      = {arXiv Fulltext PDF:C\:\\Users\\fgerv\\Zotero\\storage\\WZG579BA\\Rygl et al. - 2017 - Semantic Vector Encoding and Similarity Search Usi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\PWARDLQB\\1706.html:text/html}
}

@article{eckart_approximation_1936,
  title    = {The approximation of one matrix by another of lower rank},
  volume   = {1},
  issn     = {1860-0980},
  url      = {https://doi.org/10.1007/BF02288367},
  doi      = {10.1007/BF02288367},
  abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.},
  language = {en},
  number   = {3},
  urldate  = {2023-04-06},
  journal  = {Psychometrika},
  author   = {Eckart, Carl and Young, Gale},
  month    = sep,
  year     = {1936},
  keywords = {Canonic Form, Lower Rank, Mathematical Problem, Public Policy, Statistical Theory},
  pages    = {211--218},
  file     = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\9HICRCED\\Eckart and Young - 1936 - The approximation of one matrix by another of lowe.pdf:application/pdf}
}

@inproceedings{wong_generalized_1985,
  address   = {New York, NY, USA},
  series    = {{SIGIR} '85},
  title     = {Generalized vector spaces model in information retrieval},
  isbn      = {978-0-89791-159-7},
  url       = {https://dl.acm.org/doi/10.1145/253495.253506},
  doi       = {10.1145/253495.253506},
  abstract  = {In information retrieval, it is common to model index terms and documents as vectors in a suitably defined vector space. The main difficulty with this approach is that the explicit representation of term vectors is not known a priori. For this reason, the vector space model adopted by Salton for the SMART system treats the terms as a set of orthogonal vectors. In such a model it is often necessary to adopt a separate, corrective procedure to take into account the correlations between terms. In this paper, we propose a systematic method (the generalized vector space model) to compute term correlations directly from automatic indexing scheme. We also demonstrate how such correlations can be included with minimal modification in the existing vector based information retrieval systems. The preliminary experimental results obtained from the new model are very encouraging.},
  urldate   = {2023-04-06},
  booktitle = {Proceedings of the 8th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
  publisher = {Association for Computing Machinery},
  author    = {Wong, S. K. M. and Ziarko, Wojciech and Wong, Patrick C. N.},
  month     = jun,
  year      = {1985},
  pages     = {18--25},
  file      = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\JMUXZIKK\\Wong et al. - 1985 - Generalized vector spaces model in information ret.pdf:application/pdf}
}

@inproceedings{isbell_restructuring_1998,
  title     = {Restructuring {Sparse} {High} {Dimensional} {Data} for {Effective} {Retrieval}},
  volume    = {11},
  url       = {https://proceedings.neurips.cc/paper_files/paper/1998/hash/87ec2f451208df97228105657edb717f-Abstract.html},
  abstract  = {The task in text retrieval is to find  the subset of a collection of documents relevant  to  a user's  information request,  usually expressed as  a set of words.  Classically,  documents and queries are represented as  vectors of word counts.  In  its  simplest  form, relevance is  defined to be the dot product between a document and  a query  vector-a measure  of the  number of common  terms.  A  central  difficulty  in  text  retrieval  is  that  the  presence or absence  of a  word  is  not sufficient to  determine  relevance to a query. Linear dimensionality reduction has been proposed as a tech(cid:173) nique for extracting underlying structure from  the document collection.  In  some  domains  (such  as  vision)  dimensionality  reduction  reduces  computational com(cid:173) plexity.  In  text retrieval  it is  more often  used  to  improve retrieval  performance.  We  propose  an  alternative  and  novel  technique  that  produces  sparse  represen(cid:173) tations  constructed  from  sets  of highly-related  words.  Documents  and  queries  are  represented  by  their distance  to  these  sets,  and relevance  is  measured by  the  number of common clusters.  This technique significantly improves retrieval  per(cid:173) formance,  is  efficient  to  compute  and  shares  properties  with  the  optimal  linear  projection operator and the independent components of documents.},
  urldate   = {2023-04-06},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {MIT Press},
  author    = {Isbell, Charles and Viola, Paul},
  year      = {1998},
  file      = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\DSSW5H6B\\Isbell and Viola - 1998 - Restructuring Sparse High Dimensional Data for Eff.pdf:application/pdf}
}

@misc{koech_cross-entropy_2022,
  title    = {Cross-{Entropy} {Loss} {Function}},
  url      = {https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e},
  abstract = {A loss function used in most classification problems to optimize machine learning model…},
  language = {en},
  urldate  = {2023-04-06},
  journal  = {Medium},
  author   = {Koech, Kiprono Elijah},
  month    = jul,
  year     = {2022}
}

@misc{arduino_mega,
  title    = {Arduino {Mega} 2560 {Rev3}},
  url      = {https://store.arduino.cc/products/arduino-mega-2560-rev3},
  abstract = {The Arduino Mega 2560 is a microcontroller board based on the ATmega2560. It has 54 digital input/output pins (of which 15 can be used as PWM outputs), 16 analog inputs, 4 UARTs (hardware serial ports), a 16 MHz crystal oscillator, a USB connection, a power jack, an ICSP header, and a reset button. It contains everythi},
  language = {en},
  urldate  = {2023-04-07},
  journal  = {Arduino Official Store},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\93RB5BBW\\arduino-mega-2560-rev3.html:text/html},
  author   = {Arduino}
}

@misc{pyttsx3,
  title   = {pyttsx3 - {Text}-to-speech x-platform — pyttsx3 2.6 documentation},
  url     = {https://pyttsx3.readthedocs.io/en/latest/index.html},
  urldate = {2023-04-07},
  file    = {pyttsx3 - Text-to-speech x-platform — pyttsx3 2.6 documentation:C\:\\Users\\fgerv\\Zotero\\storage\\4TBHKUYW\\index.html:text/html},
  author  = {pyttsx3}
}

@misc{wiring,
  title   = {Wiring},
  url     = {http://wiring.org.co/},
  urldate = {2023-04-10},
  file    = {Wiring:C\:\\Users\\fgerv\\Zotero\\storage\\DPPFSN9X\\wiring.org.co.html:text/html},
  author  = {Wiring}
}

@misc{arduino_education,
  title    = {Arduino {Education}},
  url      = {https://www.arduino.cc/education},
  abstract = {Arduino Education creates the next generation of STEAM programs that empower students on their learning journey through middle school, high school, and university and help them thrive.},
  language = {en},
  urldate  = {2023-04-10},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\HHTCA85T\\education.html:text/html},
  author   = {Arduino}
}

@misc{arduino_language,
  title   = {Arduino {Reference} - {Arduino} {Reference}},
  url     = {https://www.arduino.cc/reference/en/},
  urldate = {2023-04-10},
  file    = {Arduino Reference - Arduino Reference:C\:\\Users\\fgerv\\Zotero\\storage\\F3MZ9P76\\en.html:text/html},
  author  = {Arduino}
}

@misc{pwm,
  title    = {Basics of {PWM} ({Pulse} {Width} {Modulation}) {\textbar} {Arduino} {Documentation}},
  url      = {https://docs.arduino.cc/learn/microcontrollers/analog-output},
  abstract = {Learn how PWM works and how to use it in a sketch..},
  urldate  = {2023-04-11},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\MD2PR862\\analog-output.html:text/html},
  author   = {Arduino}
}

@misc{zhou_home_nodate,
  title    = {Home},
  url      = {https://nextion.tech/},
  abstract = {NEXTION INTRODUCTION 
              What's Nextion  Nextion is a Human Machine Interface （HMI） solution combining an onboard processor and memory touch display with Nextion Editor},
  language = {en-US},
  urldate  = {2023-04-11},
  journal  = {Nextion},
  author   = {Zhou, Baojie},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\B2GA2LNH\\nextion.tech.html:text/html}
}

@book{janarthanam_hands-chatbots_2017,
  address    = {Birmingham Mumbai},
  title      = {Hands-{On} {Chatbots} and {Conversational} {UI} {Development}: {Build} chatbots and voice user interfaces with {Chatfuel}, {Dialogflow}, {Microsoft} {Bot} {Framework}, {Twilio}, and {Alexa} {Skills}},
  isbn       = {978-1-78829-466-9},
  shorttitle = {Hands-{On} {Chatbots} and {Conversational} {UI} {Development}},
  abstract   = {Build over 8 chatbots and conversational user interfaces with leading tools such as Chatfuel, Dialogflow, Microsoft Bot Framework, Twilio, Alexa Skills, and Google Actions and deploying them on channels like Facebook Messenger, Amazon Alexa and Google HomeKey FeaturesUnderstand the different use cases of Conversational UIs with this project-based guideBuild feature-rich Chatbots and deploy them on multiple platformsGet real-world examples of voice-enabled UIs for personal and home assistanceBook DescriptionConversation as an interface is the best way for machines to interact with us using the universally accepted human tool that is language. Chatbots and voice user interfaces are two flavors of conversational UIs. Chatbots are real-time, data-driven answer engines that talk in natural language and are context-aware. Voice user interfaces are driven by voice and can understand and respond to users using speech. This book covers both types of conversational UIs by leveraging APIs from multiple platforms. We'll take a project-based approach to understand how these UIs are built and the best use cases for deploying them. We'll start by building a simple messaging bot from the Facebook Messenger API to understand the basics of bot building. Then we move on to creating a Task model that can perform complex tasks such as ordering and planning events with the newly-acquired-by-Google Dialogflow and Microsoft Bot framework. We then turn to voice-enabled UIs that are capable of interacting with users using speech with Amazon Alexa and Google Home. By the end of the book, you will have created your own line of chatbots and voice UIs for multiple leading platforms.What you will learnDesign the flow of conversation between the user and the chatbotCreate Task model chatbots for implementing tasks such as ordering foodGet new toolkits and services in the chatbot ecosystemIntegrate third-party information APIs to build interesting chatbots Find out how to deploy chatbots on messaging platformsBuild a chatbot using MS Bot FrameworkSee how to tweet, listen to tweets, and respond using a chatbot on TwitterPublish chatbots on Google Assistant and Amazon AlexaWho This Book Is ForThis book is for developers who are interested in creating interactive conversational UIs/Chatbots. A basic understanding of JavaScript and web APIs is required.Table of ContentsIntroductionLet's take a walkTalking about weatherBuild your own CV botBuild a journey planner bot Build a food ordering botMake your own news station botWhats on TVBuilding a Kitchen Bot},
  language   = {Englisch},
  publisher  = {Packt Publishing},
  author     = {Janarthanam, Srini},
  year       = {2017}
}

@book{bird_natural_2009,
  address    = {Beijing ; Cambridge Mass.},
  edition    = {1},
  title      = {Natural {Language} {Processing} with {Python}: {Analyzing} {Text} with the {Natural} {Language} {Toolkit}},
  isbn       = {978-0-596-51649-9},
  shorttitle = {Natural {Language} {Processing} with {Python}},
  abstract   = {This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication.Packed with examples and exercises, Natural Language Processing with Python will help you:* Extract information from unstructured text, either to guess the topic or identify "named entities"* Analyze linguistic structure in text, including parsing and semantic analysis* Access popular linguistic databases, including WordNet and treebanks* Integrate techniques drawn from fields as diverse as linguistics and artificial intelligenceThis book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.},
  language   = {Englisch},
  publisher  = {O'Reilly and Associates},
  author     = {Bird, Steven and Klein, Ewan and Loper, Edward},
  month      = jul,
  year       = {2009}
}

@book{mccarthy_applied_2011,
  address    = {Hershey, PA},
  edition    = {1},
  title      = {Applied {Natural} {Language} {Processing}: {Identification}, {Investigation} and {Resolution}},
  isbn       = {978-1-60960-741-8},
  shorttitle = {Applied {Natural} {Language} {Processing}},
  abstract   = {The amount of information that humans have gathered and made available to other humans is phenomenal, yet however large this repository of knowledge is, by this time tomorrow, it will be larger still.Applied Natural Language Processing: Identification, Investigation and Resolution is a volume dedicated to the successful application of processing tools to this information. The majority of this knowledge is expressed through textual media, which requires these tools to utilize the research in the field of Applied Natural Language Processing. This book contains state-of-the-art Applied Natural Language Processing techniques as well as their applications. The descriptions and studies are the product of established researchers in fields relating to ANLP, and this work is relevant to teachers, students, and materials developers in fields spanning computer science, linguistics, and cognitive science.},
  language   = {Englisch},
  publisher  = {IGI Global},
  author     = {McCarthy, Philip M.},
  editor     = {Boonthum-Denecke, Chutima},
  year       = {2011}
}

@book{manning_foundations_1999,
  address   = {Cambridge, Mass},
  edition   = {1},
  title     = {Foundations of {Statistical} {Natural} {Language} {Processing}},
  isbn      = {978-0-262-13360-9},
  abstract  = {Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.},
  language  = {Englisch},
  publisher = {The MIT Press},
  author    = {Manning, Christopher D. and Schütze, Hinrich},
  month     = jun,
  year      = {1999}
}

@misc{nltk_wordnet,
  title   = {{NLTK} :: nltk.stem.wordnet},
  url     = {https://www.nltk.org/_modules/nltk/stem/wordnet.html},
  urldate = {2023-04-11},
  file    = {NLTK \:\: nltk.stem.wordnet:C\:\\Users\\fgerv\\Zotero\\storage\\8C8FZII6\\wordnet.html:text/html},
  author  = {NLTK}
}

@misc{numpy,
  title   = {{NumPy}},
  url     = {https://numpy.org/},
  urldate = {2023-04-11},
  file    = {NumPy:C\:\\Users\\fgerv\\Zotero\\storage\\D3B72MWU\\numpy.org.html:text/html},
  author  = {NumPy}
}

@misc{weisstein_l2-norm_nodate,
  type      = {Text},
  title     = {L{\textasciicircum}2-{Norm}},
  copyright = {Copyright 1999-2023 Wolfram Research, Inc.  See https://mathworld.wolfram.com/about/terms.html for a full terms of use statement.},
  url       = {https://mathworld.wolfram.com/},
  abstract  = {The l{\textasciicircum}2-norm (also written "l{\textasciicircum}2-norm") {\textbar}x{\textbar} is a vector norm defined for a complex vector  x=[x\_1; x\_2; {\textbar}; x\_n]  (1)   by  {\textbar}x{\textbar}=sqrt(sum\_(k=1){\textasciicircum}n{\textbar}x\_k{\textbar}{\textasciicircum}2),  (2)   where {\textbar}x\_k{\textbar} on the right denotes the complex modulus. The l{\textasciicircum}2-norm is the vector norm that is commonly encountered in vector algebra and vector operations (such as the dot product), where it is commonly denoted {\textbar}x{\textbar}. However, if desired, a more explicit (but more cumbersome) notation {\textbar}x{\textbar}\_2 can be used to emphasize the...},
  language  = {en},
  urldate   = {2023-04-11},
  author    = {Weisstein, Eric W.},
  note      = {Publisher: Wolfram Research, Inc.},
  file      = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\VFSXSQSB\\L2-Norm.html:text/html}
}

@book{han_data_2011,
  edition    = {3},
  title      = {Data {Mining}: {Concepts} and {Techniques}},
  isbn       = {978-93-80931-91-3},
  shorttitle = {Data {Mining}},
  abstract   = {Data Mining: Concepts and Techniques provides the concepts and techniques in processing gathered data or information, which will be used in various applications. Specifically, it explains data mining and the tools used in discovering knowledge from the collected data. This book is referred as the knowledge discovery from data (KDD). It focuses on the feasibility, usefulness, effectiveness, and scalability of techniques of large data sets. After describing data mining, this edition explains the methods of knowing, preprocessing, processing, and warehousing data. It then presents information about data warehouses, online analytical processing (OLAP), and data cube technology. Then, the methods involved in mining frequent patterns, associations, and correlations for large data sets are described. The book details the methods for data classification and introduces the concepts and methods for data clustering. The remaining chapters discuss the outlier detection and the trends, applications, and research frontiers in data mining.This book is intended for Computer Science students, application developers, business professionals, and researchers who seek information on data mining.},
  language   = {Englisch},
  publisher  = {Morgan Kaufmann},
  author     = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  month      = jul,
  year       = {2011}
}

@book{manning_introduction_2008,
  address   = {New York},
  edition   = {Anniversary Edition},
  title     = {Introduction to {Information} {Retrieval}},
  isbn      = {978-0-521-86571-5},
  abstract  = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
  language  = {Englisch},
  publisher = {Cambridge University Press},
  author    = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  month     = jul,
  year      = {2008}
}

@inproceedings{dumais_using_1988,
  address   = {New York, NY, USA},
  series    = {{CHI} '88},
  title     = {Using latent semantic analysis to improve access to textual information},
  isbn      = {978-0-201-14237-2},
  url       = {https://dl.acm.org/doi/10.1145/57167.57214},
  doi       = {10.1145/57167.57214},
  abstract  = {This paper describes a new approach for dealing with the vocabulary problem in human-computer interaction. Most approaches to retrieving textual materials depend on a lexical match between words in users' requests and those in or assigned to database objects. Because of the tremendous diversity in the words people use to describe the same object, lexical matching methods are necessarily incomplete and imprecise [5]. The latent semantic indexing approach tries to overcome these problems by automatically organizing text objects into a semantic structure more appropriate for matching user requests. This is done by taking advantage of implicit higher-order structure in the association of terms with text objects. The particular technique used is singular-value decomposition, in which a large term by text-object matrix is decomposed into a set of about 50 to 150 orthogonal factors from which the original matrix can be approximated by linear combination. Terms and objects are represented by 50 to 150 dimensional vectors and matched against user queries in this “semantic” space. Initial tests find this completely automatic method widely applicable and a promising way to improve users' access to many kinds of textual materials, or to objects and services for which textual descriptions are available.},
  urldate   = {2023-04-11},
  booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
  publisher = {Association for Computing Machinery},
  author    = {Dumais, S. T. and Furnas, G. W. and Landauer, T. K. and Deerwester, S. and Harshman, R.},
  month     = may,
  year      = {1988},
  pages     = {281--285},
  file      = {Full Text PDF:C\:\\Users\\fgerv\\Zotero\\storage\\V9NZZ2MD\\Dumais et al. - 1988 - Using latent semantic analysis to improve access t.pdf:application/pdf}
}

@book{goodfellow_deep_2016,
  address   = {Cambridge, Massachusetts},
  title     = {Deep {Learning}},
  isbn      = {978-0-262-03561-3},
  abstract  = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  language  = {Englisch},
  publisher = {The MIT Press},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  month     = nov,
  year      = {2016}
}

@book{geron_hands-machine_2019,
  address    = {Beijing China ; Sebastopol, CA},
  edition    = {2nd ed Edition},
  title      = {Hands-on {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: {Concepts}, {Tools}, and {Techniques} to {Build} {Intelligent} {Systems}},
  isbn       = {978-1-4920-3264-9},
  shorttitle = {Hands-on {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}},
  abstract   = {{\textless}div{\textgreater}Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how.By using concrete examples, minimal theory, and two production-ready Python frameworks\&;Scikit-Learn and TensorFlow\&;author Aurélien Géron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You\&;ll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you\&;ve learned, all you need is programming experience to get started.Explore the machine learning landscape, particularly neural netsUse Scikit-Learn to track an example machine-learning project end-to-endExplore several training models, including support vector machines, decision trees, random forests, and ensemble methodsUse the TensorFlow library to build and train neural netsDive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learningLearn techniques for training and scaling deep neural nets{\textless}/div{\textgreater}},
  language   = {Englisch},
  publisher  = {O'Reilly UK Ltd.},
  author     = {Géron, Aurélien},
  year       = {2019}
}

@book{aggarwal_neural_2018,
  address    = {Cham, Switzerland},
  edition    = {1st ed. 2018 Edition},
  title      = {Neural {Networks} and {Deep} {Learning}: {A} {Textbook}},
  isbn       = {978-3-319-94462-3},
  shorttitle = {Neural {Networks} and {Deep} {Learning}},
  abstract   = {This book covers both classical and modern models in deep learning. The primary focus is on the theory and algorithms of deep learning. The theory and algorithms of neural networks are particularly important for understanding important concepts, so that one can understand the important design concepts of neural architectures in different applications. Why do neural networks work? When do they work better than off-the-shelf machine-learning models? When is depth useful? Why is training neural networks so hard? What are the pitfalls? The book  is also rich in discussing different applications in order to give the practitioner a flavor of how neural architectures are designed for different types of problems. Applications associated with many different areas like recommender systems, machine translation, image captioning, image classification, reinforcement-learning based gaming, and text analytics are covered. The chapters of this book span three categories:The basics of neural networks:  Many traditional machine learning models can be understood as special cases of neural networks.  An emphasis is placed in the first two chapters on understanding the relationship between traditional machine learning and neural networks. Support vector machines, linear/logistic regression, singular value decomposition, matrix factorization, and recommender systems are shown to be special cases of neural networks. These methods are studied together with recent feature engineering methods like word2vec.Fundamentals of neural networks: A detailed discussion of training and regularization is provided in Chapters 3 and 4. Chapters 5 and 6 present radial-basis function (RBF) networks and restricted Boltzmann machines.Advanced topics in neural networks: Chapters 7 and 8 discuss recurrent neural networks and convolutional neural networks. Several advanced topics like deep reinforcement learning, neural Turing machines, Kohonen self-organizing maps, and generative adversarial networks are introduced in Chapters 9 and 10.The book is written for graduate students, researchers, and practitioners.   Numerous exercises are available along with a solution manual to aid in classroom teaching. Where possible, an application-centric view is highlighted in order to provide an understanding of the practical uses of each class of techniques.},
  language   = {Englisch},
  publisher  = {Springer},
  author     = {Aggarwal, Charu C.},
  month      = sep,
  year       = {2018}
}

@book{raschka_python_2017,
  address    = {Birmingham Mumbai},
  edition    = {Second Edition},
  title      = {Python {Machine} {Learning} - {Second} {Edition}: {Machine} {Learning} and {Deep} {Learning} with {Python}, scikit-learn, and {TensorFlow}},
  isbn       = {978-1-78712-593-3},
  shorttitle = {Python {Machine} {Learning} - {Second} {Edition}},
  abstract   = {Publisher's Note: This edition from 2017 is outdated and is not compatible with TensorFlow 2 or any of the most recent updates to Python libraries. A new third edition, updated for 2020 and featuring TensorFlow 2 and the latest in scikit-learn, reinforcement learning, and GANs, has now been published.Key FeaturesSecond edition of the bestselling book on Machine LearningA practical approach to key frameworks in data science, machine learning, and deep learningUse the most powerful Python libraries to implement machine learning and deep learningGet to know the best practices to improve and optimize your machine learning systems and algorithmsBook DescriptionMachine learning is eating the software world, and now deep learning is extending machine learning. Understand and work at the cutting edge of machine learning, neural networks, and deep learning with this second edition of Sebastian Raschka's bestselling book, Python Machine Learning. Using Python's open source libraries, this book offers the practical knowledge and techniques you need to create and contribute to machine learning, deep learning, and modern data analysis.Fully extended and modernized, Python Machine Learning Second Edition now includes the popular TensorFlow 1.x deep learning library. The scikit-learn code has also been fully updated to v0.18.1 to include improvements and additions to this versatile machine learning library.Sebastian Raschka and Vahid Mirjalili's unique insight and expertise introduce you to machine learning and deep learning algorithms from scratch, and show you how to apply them to practical industry challenges using realistic and interesting examples. By the end of the book, you'll be ready to meet the new data analysis opportunities.If you've read the first edition of this book, you'll be delighted to find a balance of classical ideas and modern insights into machine learning. Every chapter has been critically updated, and there are new chapters on key technologies. You'll be able to learn and work with TensorFlow 1.x more deeply than ever before, and get essential coverage of the Keras neural network library, along with updates to scikit-learn 0.18.1.What you will learnUnderstand the key frameworks in data science, machine learning, and deep learningHarness the power of the latest Python open source libraries in machine learningExplore machine learning techniques using challenging real-world dataMaster deep neural network implementation using the TensorFlow 1.x libraryLearn the mechanics of classification algorithms to implement the best tool for the jobPredict continuous target outcomes using regression analysisUncover hidden patterns and structures in data with clusteringDelve deeper into textual and social media data using sentiment analysis},
  language   = {Englisch},
  publisher  = {Packt Publishing},
  author     = {Raschka, Sebastian and Mirjalili, Vahid},
  month      = sep,
  year       = {2017}
}

@misc{nx4832t035,
  title    = {{NX4832T035}},
  url      = {https://nextion.tech/datasheets/nx4832t035/},
  abstract = {NX4832T035 Overview Nextion Models Specifications Electronic Characteristics Working Environment \& Reliability Parameter Interfaces Performance Memory Features Product Dimensions Overview Nextion is a seamless Human Machine Interface (HMI) solution that provides a control and visualisation interface between a human and a process, machine, application or appliance. Nextion is mainly applied to IoT or consumer electronics field. […]},
  language = {en-US},
  urldate  = {2023-04-12},
  journal  = {Nextion},
  author   = {Nextion},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\DHHEDS32\\nx4832t035.html:text/html}
}

@misc{nextion_library,
  title     = {Nextion},
  copyright = {MIT},
  url       = {https://github.com/itead/ITEADLIB_Arduino_Nextion},
  urldate   = {2023-04-12},
  publisher = {ITEAD Studio},
  month     = apr,
  year      = {2023},
  note      = {original-date: 2015-05-08T01:01:02Z},
  author    = {Nextion}
}

@misc{nextion_instructions,
  title   = {Instruction {Set} - {Nextion}},
  url     = {https://nextion.tech/instruction-set/},
  urldate = {2023-04-12},
  author  = {Nextion}
}


@misc{elegoo,
  title    = {{ELEGOO} {MEGA} 2560 {R3} {Board} with {USB} {Cable} {Compatible} with {Arduino} {IDE}},
  url      = {https://www.elegoo.com/en-de/products/elegoo-mega-2560-r3-board},
  abstract = {The Mega is 100\% compatible with Arduino IDE and most of the shields are designed for the Arduino mega2560 R3，RoHS Compliant The chip is Atmega2560-16au and Atmega16u2, the same as the official version. Improved and expert version: 1. 0 pinout: added SDA and SCL pins that are near to the AREF pin Stronger RESET circuit. Use 5. 5 * 2. 1mm DC barrel jack (compatible with 5. 5 * 2. 5mm DC barrel jack) It contains everything needed to support the microcontroller; simply connect it to a computer with a USB cable or power it with an AC-to-DC adapter or battery to get started.},
  language = {en},
  urldate  = {2023-04-11},
  journal  = {ELEGOO Official},
  author   = {Elegoo}
}

@article{deerwester_indexing_1990,
  title    = {Indexing by latent semantic analysis},
  volume   = {41},
  issn     = {1097-4571},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
  doi      = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
  abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
  language = {en},
  number   = {6},
  urldate  = {2023-04-06},
  journal  = {Journal of the American Society for Information Science},
  author   = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  year     = {1990},
  note     = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-4571\%28199009\%2941\%3A6\%3C391\%3A\%3AAID-ASI1\%3E3.0.CO\%3B2-9},
  pages    = {391--407},
  file     = {Snapshot:C\:\\Users\\fgerv\\Zotero\\storage\\RMW6UW57\\(SICI)1097-4571(199009)416391AID-ASI13.0.html:text/html;Submitted Version:C\:\\Users\\fgerv\\Zotero\\storage\\JAUBXCZZ\\Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:application/pdf}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
